{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b838e62",
   "metadata": {},
   "source": [
    "# Data Analysis\n",
    "\n",
    "In this notebook, we'll put together many of the skills we've learned so far to analyze a set of .csv files containing inflammation data.\n",
    "\n",
    "![](https://swcarpentry.github.io/python-novice-inflammation/fig/lesson-overview.svg)\n",
    "\n",
    "## About this notebook\n",
    "This notebook is based on [this notebook](https://swcarpentry.github.io/python-novice-inflammation/02-numpy/index.html) from the Software Carpentries and is licensed under a CC-BY-40 license (2018-2021):\n",
    "\n",
    "> Azalee Bostroem, Trevor Bekolay, and Valentina Staneva (eds):\n",
    "\"Software Carpentry: Programming with Python.\"  Version 2016.06, June\n",
    "2016, https://github.com/swcarpentry/python-novice-inflammation,\n",
    "10.5281/zenodo.57492.\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0372fc1",
   "metadata": {},
   "source": [
    "As a final piece to processing our inflammation data, we need a way to get a list of all the files in our data directory whose names start with inflammation- and end with .csv. The following library will help us to achieve this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326f263e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e4a84d",
   "metadata": {},
   "source": [
    "The `glob` library ([Documentation](https://docs.python.org/3/library/glob.html)) contains a function, also called `glob`, that finds files and directories whose names match a pattern. We provide those patterns as strings: the character `*` matches zero or more characters, while `?` matches any one character. We can use this to get the names of all the inflammation CSV files in our data directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390c0dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(glob.glob('Data/inflammation/inflammation*.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097f6579",
   "metadata": {},
   "source": [
    "As these examples show, `glob.glob`’s result is a list of file and directory paths in arbitrary order. This means we can loop over it to do something with each filename in turn. In our case, the “something” we want to do is generate a set of plots for each file in our inflammation dataset.\n",
    "\n",
    "If we want to start by analyzing just the first three files in alphabetical order, we can use the `sort` method to sort the `glob.glob` output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46c6ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = glob.glob('Data/inflammation/inflammation*.csv')\n",
    "filenames.sort()\n",
    "filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8eee8e",
   "metadata": {},
   "source": [
    "Now, we'll loop through those first three files and plot the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51729824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "first_three_files = filenames[0:3]                      # Step 1\n",
    "\n",
    "for filename in first_three_files:                      # Step 2\n",
    "    \n",
    "    print(filename)\n",
    "\n",
    "    data = np.loadtxt(fname=filename, delimiter=',')    # Step 3\n",
    "\n",
    "    fig,ax = plt.subplots(1,3,figsize=(10.0, 3.0))      # Step 4\n",
    "\n",
    "    ax[0].set_ylabel('average')                         # Step 5\n",
    "    ax[0].plot(np.mean(data, axis=0))\n",
    "\n",
    "    ax[1].set_ylabel('max')\n",
    "    ax[1].plot(np.max(data, axis=0))\n",
    "\n",
    "    ax[2].set_ylabel('min')\n",
    "    ax[2].plot(np.min(data, axis=0))\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d154be0e",
   "metadata": {},
   "source": [
    "The plots generated for the second clinical trial file look very similar to the plots for the first file: their average plots show similar “noisy” rises and falls; their maxima plots show exactly the same linear rise and fall; and their minima plots show similar staircase structures.\n",
    "\n",
    "The third dataset shows much noisier average and maxima plots that are far less suspicious than the first two datasets, however the minima plot shows that the third dataset minima is consistently zero across every day of the trial. If we produce a heat map for the third data file we see the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99e07eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data for third data file\n",
    "data = np.loadtxt(fname=filenames[2], delimiter=',')\n",
    "\n",
    "# Plot heatmap\n",
    "plt.imshow(data)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66a8bfc",
   "metadata": {},
   "source": [
    "Looking closely, we can see that there are zero values sporadically distributed across all patients and days of the clinical trial, suggesting that there were potential issues with data collection throughout the trial. In addition, we can see that the last patient in the study didn’t have any inflammation flare-ups at all throughout the trial, suggesting that they may not even suffer from arthritis!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3de630",
   "metadata": {},
   "source": [
    "### Task 1: Plotting differences\n",
    "> Plot the difference between the average inflammations (in other words, one mean minus another mean) reported in the first and second datasets (stored in inflammation-01.csv and inflammation-02.csv, correspondingly). In other words, plot the difference between the leftmost plots of the first two figures above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1ed0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load first two data sets\n",
    "\n",
    "# Calculate difference in means\n",
    "\n",
    "# Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ec676c",
   "metadata": {},
   "source": [
    "### Task 2: Generate composite statistics\n",
    "\n",
    "> 1. Use each of the files once to generate a dataset containing values averaged over all patients. There is a skeleton for this below.\n",
    "> 2. Use `plt.plot()` to generate plots of average, max, and min for *all* patients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce244db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = glob.glob('inflammation*.csv')\n",
    "\n",
    "composite_data = np.zeros((60,40))\n",
    "\n",
    "for filename in filenames:\n",
    "    # sum each new file's data into composite_data as it's read\n",
    "    #\n",
    "\n",
    "    # and then divide the composite_data by number of samples\n",
    "    \n",
    "composite_data = composite_data / len(filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787c14e6",
   "metadata": {},
   "source": [
    "After spending some time investigating the heat map and statistical plots, as well as doing the above exercises to plot differences between datasets and to generate composite patient statistics, we gain some insight into the twelve clinical trial datasets.\n",
    "\n",
    "The datasets appear to fall into two categories:\n",
    "\n",
    "1. seemingly “ideal” datasets that agree excellently with Dr. Maverick’s claims, but display suspicious maxima and minima (such as inflammation-01.csv and inflammation-02.csv)\n",
    "2. “noisy” datasets that somewhat agree with Dr. Maverick’s claims, but show concerning data collection issues such as sporadic missing values and even an unsuitable candidate making it into the clinical trial.\n",
    "\n",
    "In fact, it appears that all three of the “noisy” datasets (inflammation-03.csv, inflammation-08.csv, and inflammation-11.csv) are identical down to the last value. Armed with this information, we confront Dr. Maverick about the suspicious data and duplicated files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094be9d6",
   "metadata": {},
   "source": [
    "<img src=\"https://c.tenor.com/Pkj5t7gMBoIAAAAC/scrunch-suspicious.gif\" alt=\"gif\" width=\"400\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d69aad7",
   "metadata": {},
   "source": [
    "Dr. Maverick confesses that they fabricated the clinical data after they found out that the initial trial suffered from a number of issues, including unreliable data-recording and poor participant selection. They created fake data to prove their drug worked, and when we asked for more data they tried to generate more fake datasets, as well as throwing in the original poor-quality dataset a few times to try and make all the trials seem a bit more “realistic”.\n",
    "\n",
    "Congratulations! We’ve investigated the inflammation data and proven that the datasets have been synthetically generated. Let's dig in a little further though."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3eebee",
   "metadata": {},
   "source": [
    "## What's up with our data?\n",
    "Below, we'll use conditionals to inspect the suspicious features we saw in our inflammation data.\n",
    "\n",
    "From the first couple of plots we generated above, we saw that maximum daily inflammation exhibits a strange behavior and raises one unit a day. Wouldn’t it be a good idea to detect such behavior and report it as suspicious? Let’s do that! However, instead of checking every single day of the study, let’s merely check if maximum inflammation in the beginning (day 0) and in the middle (day 20) of the study are equal to the corresponding day numbers. The code to do so would look like this:\n",
    "\n",
    "```\n",
    "max_inflammation_0 = np.max(data, axis=0)[0]\n",
    "max_inflammation_20 = np.max(data, axis=0)[20]\n",
    "\n",
    "if max_inflammation_0 == 0 and max_inflammation_20 == 20:\n",
    "    print('Suspicious looking maxima!')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036fac37",
   "metadata": {},
   "source": [
    "We also saw a different problem in the third dataset; the minima per day were all zero (looks like a healthy person snuck into our study). We can also check for this with an elif condition:\n",
    "\n",
    "```\n",
    "elif np.sum(np.min(data, axis=0)) == 0:\n",
    "    print('Minima add up to zero!')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1513de00",
   "metadata": {},
   "source": [
    "And if neither of these conditions are true, we can use else to give the all-clear:\n",
    "\n",
    "```\n",
    "else:\n",
    "    print('Seems OK!')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44f6590",
   "metadata": {},
   "source": [
    "### Task 3: Wrap your data cleaning into a function\n",
    "> Using the provided code above, write a function called `clean_data` which takes in a filename and performs the checks above. Write your function so that it provides the user with information about the data. For example, if the maxima are suspicious, print 'Suspicious looking maxima!'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e15a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your function here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a9fc45",
   "metadata": {},
   "source": [
    "Once we have our function, we can run it on multiple datasets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc5a1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in filenames:\n",
    "    print(filename)\n",
    "    clean_data(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c570409e",
   "metadata": {},
   "source": [
    "### Other data analysis functions\n",
    "Below, we write a function to offset a dataset so that it’s mean value shifts to a user-defined value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b74f2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def offset_mean(data, target_mean_value):\n",
    "    \n",
    "    '''Return a new array containing the original data\n",
    "       with its mean offset to match the desired value.'''\n",
    "    \n",
    "    return (data - np.mean(data)) + target_mean_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1da166",
   "metadata": {},
   "source": [
    "Let’s try `offset_mean` on our real inflammation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0fbe0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that our function works\n",
    "data = np.loadtxt(fname='Data/inflammation/inflammation-01.csv', delimiter=',')\n",
    "\n",
    "offset_mean(data, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7ba066",
   "metadata": {},
   "source": [
    "It’s hard to tell from the default output whether the result is correct, but there are a few tests that we can run to reassure us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eea6f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('original min, mean, and max are:', np.min(data), np.mean(data), np.max(data))\n",
    "\n",
    "offset_data = offset_mean(data, 0)\n",
    "\n",
    "print('min, mean, and max of offset data are:',\n",
    "      np.min(offset_data),\n",
    "      np.mean(offset_data),\n",
    "      np.max(offset_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c61476",
   "metadata": {},
   "source": [
    "That seems almost right: the original mean was about 6.1, so the lower bound from zero is now about -6.1. The mean of the offset data isn’t quite zero, but it’s pretty close."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a005ff6",
   "metadata": {},
   "source": [
    "### Side note: setting default values in our function\n",
    "Finally, if we wanted to write this function so that we don't *need* to give it a `target_mean_value`, we can set a default value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6e62ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def offset_mean(data, target_mean_value=0.0):\n",
    "    \"\"\"Return a new array containing the original data\n",
    "       with its mean offset to match the desired value, (0 by default).\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> offset_mean([1, 2, 3])\n",
    "    array([-1.,  0.,  1.])\n",
    "    \"\"\"\n",
    "    return (data - np.mean(data)) + target_mean_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0078d69",
   "metadata": {},
   "source": [
    "The key change is that the second parameter is now written `target_mean_value=0.0` instead of just `target_mean_value`. If we call the function with two arguments, it works as it did before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a186363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test our function\n",
    "offset_mean(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ec08b4",
   "metadata": {},
   "source": [
    "Nicely done, now you're ready to analyze lots of data!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
